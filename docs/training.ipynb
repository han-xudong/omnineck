{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guideline: Simulating and Training for the OmniNeck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a guide on how to simulate the OmniNeck in Abaqus to generate the dataset for training the NeckNet model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some necessary packages and functions needed to be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import re\n",
    "import shutil\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from omnineck.models import NeckNetRuntime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input File Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.inp` file is necessary for Abaqus simulation. It contains the information about the geometry, material properties, boundary conditions, history and field outputs, etc. Here we provide typical steps to create an `.inp` file template for the OmniNeck.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After opening the Abaqus CAE, we create a new model database with standard/explicit model. Then we import the `omnineck.STEP` file to create a part. The part can be named as `omnineck`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `Property` module, we create a new material `PU`, select `Mechanical` -> `Elasticity` -> `Elastic`, and input `Young's modulus=0.2` and `Poisson's ratio=0.4`. Then we create a new `Section` with `Solid` -> `Homogeneous` type, select the `Material` we just created. After that, we need to assign the `Section` to the part by selecting the part and clicking `Assign Section`. Now the material properties are defined, and the color of the part becomes green.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `Assembly` module, we can add the part to the assembly by selecting `Instance` -> `Create` -> `Part` -> `OK`. Note that we set `Instance Type` as `Dependent (mesh on part)`. The axis origin is at the center of the bottom surface of the cylinder, which is convenient for the following steps.\n",
    "\n",
    "And we need to define several sets, reference points, and surfaces:\n",
    "\n",
    "- `RP-1`: create a reference point at (0, 0, 25.75), representing the center of the marker.\n",
    "- `Set-RP-1`: create a set and select the reference point `RP-1`.\n",
    "- `Set-surface`: create a set and select all surfaces of the OmniNeck.\n",
    "- `Set-bottom`: create a set and select the bottom surface of the OmniNeck.\n",
    "- `Set-zero`: create a set and select the surfaces on which displacements are zero.\n",
    "- `Surf-bottom`: create a surface and select the bottom surface of the OmniNeck.\n",
    "- `Surf-bound`: create a surface and select the lower layer of the OmniNeck, which is bounded by the marker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `Step` module, we create a new `Step` with `Static, General` type, and set `Nlgeom` as `On`. Then we need to edit the `Field Output` and select `S, U` as the output variables. For the `History Output`, the domain needs to be selected as `Integrated output section` which needs to be created with `Surf-bottom`, and the output variables are `SOF, SOM`. Since we mainly consider the geometry and force information, other output variables can be ignored. If you are interested in other variables, you can add them as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `Interaction` module, we need to define the constraints between `RP-1` and `Surf-bound`. `Coupling` type `Constraint` is created with `Set-RP-1` selected as `Control points` and `Surf-bound` selected as `Surface`. The `Coupling type` is set as `Kinematic`, and the `Constrained degrees of freedom` are set as all. Therefore, the `Surf-bound` is slaved by changing the position of `RP-1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `Load` module, we create two boundary conditions. We first create a `BC-bottom` with `Symmetry/Antisymmetry/Encastre` type at `Initial` step, and select `Set-bottom` as the `Region` and `ENCASTRE` as the `Boundary condition`. Then we create a `BC-RP` with `Displacement/Rotation` type at `Step-1` step, and select `Set-RP-1` as the `Region` and all degrees of freedom with `0` or other values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `Mesh` module, we need to first select the `Mesh controls` and set the `Element shape` as `Tet`. Then we need to make the element type as `C3D4` by selecting `Mesh` -> `Element type` and changing the `Geometric Order` as `Linear`, and the color of the geometry becomes pink. It's optional to refine the mesh by selecting `Seed part` and setting the `Approximate global size`, `Maximum deviation factor`, and `Minimum size`. Then we generate the mesh by selecting `Mesh part`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all above steps, we can create the job to generate the `.inp` file. We can select `Create` in `Job manager`, and click the `Data Check` to check whether there are any errors. If there is no error, we can click `Write input` to generate the `.inp` file. The `.inp` file is saved in the working directory, and we moved it to the `template` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the mesh using `plotly`. The `node.txt`, `element.txt`, `surface_node.txt`, and `zero_node.txt` in the `./template/` folder are copied from the `omnineck.inp` file. The `surface_triangle.txt`, `surface_coordinate.txt`, and `deform_node.txt` are generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to define the type of the metaball:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the omnineck type\n",
    "omnineck_type = \"omnineck\"\n",
    "\n",
    "# Define the template directory\n",
    "template_dir = os.path.join(\"..\", \"templates\", omnineck_type)\n",
    "if not os.path.exists(template_dir):\n",
    "    raise FileNotFoundError(\n",
    "        f\"⚠️ Template directory '{template_dir}' does not exist. Please ensure the template is available.\"\n",
    "    )\n",
    "    \n",
    "# Define the data directory\n",
    "data_dir = os.path.join(\"..\", \"data\", omnineck_type, \"sim\")\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we extract the node and element information from the `.inp` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the inp file path\n",
    "inp_path=os.path.join(template_dir, f'{omnineck_type}.inp')\n",
    "# Check if the input file exists\n",
    "if not os.path.exists(inp_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"⚠️ Input file '{inp_path}' does not exist. Please ensure the input file is available.\"\n",
    "    )\n",
    "\n",
    "# Function to check if a line is a valid node line\n",
    "def is_valid_node_line(line) -> bool:\n",
    "    parts = line.split(',')\n",
    "    return len(parts) >= 4 and parts[0].strip().isdigit()\n",
    "\n",
    "# Function to check if a line is a valid element line\n",
    "def is_valid_elem_line(line) -> bool:\n",
    "    parts = line.split(',')\n",
    "    return len(parts) >= 2 and parts[0].strip().isdigit()\n",
    "\n",
    "# Read the inp file\n",
    "with open(inp_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Initialize lists to hold node and element lines\n",
    "node_lines = []\n",
    "elem_lines = []\n",
    "\n",
    "# Flags to track reading state\n",
    "reading_nodes = False\n",
    "reading_elems = False\n",
    "node_parsed = False\n",
    "elem_parsed = False\n",
    "\n",
    "# Process the lines to extract nodes and elements\n",
    "for line in lines:\n",
    "    striped = line.strip()\n",
    "    \n",
    "    if striped.lower().startswith('*node') and not node_parsed:\n",
    "        reading_nodes = True\n",
    "        node_parsed = True\n",
    "        reading_elems = False\n",
    "        continue\n",
    "    elif striped.lower().startswith('*element') and not elem_parsed:\n",
    "        reading_nodes = False\n",
    "        reading_elems = True\n",
    "        elem_parsed = True\n",
    "        continue\n",
    "    elif striped.startswith('*'):\n",
    "        reading_nodes = False\n",
    "        reading_elems = False\n",
    "\n",
    "    if reading_nodes:\n",
    "        if is_valid_node_line(striped):\n",
    "            node_lines.append(striped)\n",
    "        else:\n",
    "            reading_nodes = False\n",
    "            node_parsed = True\n",
    "    elif reading_elems:\n",
    "        if is_valid_elem_line(striped):\n",
    "            elem_lines.append(striped)\n",
    "        else:\n",
    "            reading_elems = False\n",
    "            elem_parsed = True\n",
    "\n",
    "# Save the node and element lines to their respective text files\n",
    "node_txt_path=os.path.join(template_dir, 'node.txt')\n",
    "with open(node_txt_path, 'w') as f:\n",
    "    for line in node_lines:\n",
    "        f.write(line + '\\n')\n",
    "element_txt_path=os.path.join(template_dir, 'element.txt')\n",
    "with open(element_txt_path, 'w') as f:\n",
    "    for line in elem_lines:\n",
    "        f.write(line + '\\n')\n",
    "print(f\"✅ Nodes saved to: {node_txt_path}\")\n",
    "print(f\"✅ Elements saved to: {element_txt_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to extract the surface node and zero node information from the `.inp` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target sets to extract\n",
    "target_sets=['Set-bound', 'Set-surface', 'Set-zero']\n",
    "\n",
    "# Function to parse generate lines\n",
    "def parse_generate(start, end, step) -> list:\n",
    "    return list(range(int(start), int(end) + 1, int(step)))\n",
    "\n",
    "# Read the inp file\n",
    "with open(inp_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Initialize a dictionary to hold nset data\n",
    "nset_data = {name: [] for name in target_sets}\n",
    "# Flags to track current set and generate state\n",
    "current_set = None\n",
    "is_generate = False\n",
    "\n",
    "# Process the lines to extract nset data\n",
    "for line in lines:\n",
    "    striped = line.strip()\n",
    "\n",
    "    if striped.lower().startswith('*nset'):\n",
    "        match = re.search(r'nset\\s*=\\s*([\\w\\-]+)', striped, re.IGNORECASE)\n",
    "        if match:\n",
    "            set_name = match.group(1)\n",
    "            if set_name in target_sets:\n",
    "                current_set = set_name\n",
    "                is_generate = 'generate' in striped.lower()\n",
    "            else:\n",
    "                current_set = None\n",
    "        continue\n",
    "\n",
    "    if striped.startswith('*'):\n",
    "        current_set = None\n",
    "        is_generate = False\n",
    "        continue\n",
    "\n",
    "    if current_set:\n",
    "        if is_generate:\n",
    "            parts = re.split(r'[,\\s]+', striped)\n",
    "            if len(parts) >= 3:\n",
    "                start, end, step = parts[:3]\n",
    "                nset_data[current_set].extend(parse_generate(start, end, step))\n",
    "        else:\n",
    "            nums = [int(s) for s in re.split(r'[,\\s]+', striped) if s.strip().isdigit()]\n",
    "            nset_data[current_set].extend(nums)\n",
    "\n",
    "# Save each nset to its respective text file\n",
    "for set_name, nodes in nset_data.items():\n",
    "    out_path = os.path.join(template_dir, f\"{set_name.split('-')[1]}_node.txt\")\n",
    "    with open(out_path, 'w') as f:\n",
    "        for node_id in sorted(set(nodes)):\n",
    "            f.write(f\"{node_id}\\n\")\n",
    "    print(f\"✅ Nset '{set_name}' saved to: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to generate the surface triangle, surface coordinate, and deform node information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for the generated files\n",
    "triangle_txt_path = os.path.join(template_dir, \"triangle.txt\")\n",
    "surface_node_txt_path = os.path.join(template_dir, \"surface_node.txt\")\n",
    "surface_triangle_txt_path = os.path.join(template_dir, \"surface_triangle.txt\")\n",
    "surface_coordinate_txt_path = os.path.join(template_dir, \"surface_coordinate.txt\")\n",
    "zero_node_txt_path = os.path.join(template_dir, \"zero_node.txt\")\n",
    "deform_node_txt_path = os.path.join(template_dir, \"deform_node.txt\")\n",
    "\n",
    "# Generate triangle.txt\n",
    "element = np.loadtxt(element_txt_path, dtype=int, delimiter=\",\")\n",
    "triangle = []\n",
    "for i in range(element.shape[0]):\n",
    "    triangle.append([element[i, 1], element[i, 2], element[i, 3]])\n",
    "    triangle.append([element[i, 1], element[i, 3], element[i, 4]])\n",
    "    triangle.append([element[i, 1], element[i, 2], element[i, 4]])\n",
    "    triangle.append([element[i, 2], element[i, 3], element[i, 4]])\n",
    "triangle = np.array(triangle, dtype=int)\n",
    "triangle = np.sort(triangle, axis=1)\n",
    "triangle = np.array(list(set([tuple(t) for t in triangle])), dtype=int)\n",
    "np.savetxt(triangle_txt_path, triangle, fmt=\"%d\", delimiter=\",\")\n",
    "print(f\"✅ Triangles saved to: {triangle_txt_path}\")\n",
    "\n",
    "# Generate surface_coordinate.txt\n",
    "surface_node = np.loadtxt(surface_node_txt_path, dtype=int)\n",
    "node = np.loadtxt(node_txt_path, dtype=float, delimiter=\",\")\n",
    "surface_coordinate = []\n",
    "for i in range(surface_node.shape[0]):\n",
    "    if node[i, 0] in surface_node:\n",
    "        surface_coordinate.append(node[i, 1:])\n",
    "surface_coordinate = np.array(surface_coordinate)\n",
    "np.savetxt(surface_coordinate_txt_path, surface_coordinate, fmt=\"%.6f\", delimiter=\",\")\n",
    "print(f\"✅ Surface node coordinates saved to: {surface_coordinate_txt_path}\")\n",
    "\n",
    "# Generate deform_node.txt\n",
    "zero_node = np.loadtxt(zero_node_txt_path, dtype=int)\n",
    "deform_node = []\n",
    "for i in range(surface_node.shape[0]):\n",
    "    if surface_node[i] not in zero_node:\n",
    "        deform_node.append(surface_node[i])\n",
    "deform_node = np.array(deform_node, dtype=int)\n",
    "np.savetxt(deform_node_txt_path, deform_node, fmt=\"%d\", delimiter=\",\")\n",
    "print(f\"✅ Deform nodes saved to: {deform_node_txt_path}\")\n",
    "\n",
    "# Generate surface_triangle.txt\n",
    "surface_triangle = []\n",
    "surface_node = surface_node.tolist()\n",
    "for i in range(triangle.shape[0]):\n",
    "    if all([triangle[i, j] in surface_node for j in range(3)]):\n",
    "        surface_triangle.append(triangle[i])\n",
    "surface_triangle = np.array(surface_triangle, dtype=int)\n",
    "np.savetxt(surface_triangle_txt_path, surface_triangle, fmt=\"%d\", delimiter=\",\")\n",
    "print(f\"✅ Surface triangles saved to: {surface_triangle_txt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can visualize the mesh in 3D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mesh\n",
    "node_ori = np.loadtxt(surface_coordinate_txt_path, delimiter=\",\")\n",
    "mesh_triangle = np.loadtxt(surface_triangle_txt_path, dtype=int, delimiter=\",\") - 1\n",
    "deform_node = np.loadtxt(deform_node_txt_path, dtype=int)\n",
    "node_curr = node_ori.copy()\n",
    "mesh3d = go.Mesh3d(\n",
    "    x=node_curr[:, 0],\n",
    "    y=node_curr[:, 1],\n",
    "    z=node_curr[:, 2],\n",
    "    i=mesh_triangle[:, 0],\n",
    "    j=mesh_triangle[:, 1],\n",
    "    k=mesh_triangle[:, 2],\n",
    "    intensity=np.linalg.norm(node_curr - node_ori, axis=1),\n",
    "    colorscale=\"Viridis\",\n",
    "    autocolorscale=False,\n",
    "    colorbar=dict(\n",
    "        title=dict(\n",
    "            text=\"Displacement (mm)\",\n",
    "            side=\"right\",\n",
    "            font=dict(size=16, family=\"Arial\"),\n",
    "        ),\n",
    "        tickvals=[0, 4, 8, 12],\n",
    "        tickfont=dict(size=14, family=\"Arial\"),\n",
    "        len=0.5,\n",
    "    ),\n",
    "    cmin=0,\n",
    "    cmax=12,\n",
    "    showscale=True,\n",
    "    flatshading=True,\n",
    ")\n",
    "fig = go.Figure(data=[mesh3d])\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        camera=dict(\n",
    "            eye=dict(x=1, y=1, z=1),\n",
    "            projection=dict(type=\"orthographic\"),\n",
    "        ),\n",
    "        xaxis=dict(nticks=4, range=[-50, 50]),\n",
    "        yaxis=dict(nticks=4, range=[-50, 50]),\n",
    "        zaxis=dict(nticks=4, range=[0, 100]),\n",
    "        aspectmode=\"manual\",\n",
    "        aspectratio=go.layout.scene.Aspectratio(x=1, y=1, z=1),\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=0, pad=0),\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate motion data of the `RP-1`, we need to define `motion_path`, representing the path to save the motion data, and `data_num`, representing the number of data to generate. Then we can generate the motion, including `u1`, `u2`, `u3`, `r1`, `r2`, `r3`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of data to generate\n",
    "data_number = 100000\n",
    "\n",
    "# Define the data directory\n",
    "data_dir = os.path.join(\"..\", \"data\", omnineck_type, \"sim\")\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Define the path to save the motion data\n",
    "motion_csv_path = os.path.join(data_dir, \"motion.csv\")\n",
    "# Generate motion.csv\n",
    "csv_file = open(motion_csv_path, \"w\", encoding=\"utf-8\", newline=\"\")\n",
    "csv_writer = csv.writer(csv_file)\n",
    "count = 0\n",
    "while count < data_number:\n",
    "    u1 = random.uniform(-10, 10)\n",
    "    u2 = random.uniform(-10, 10)\n",
    "    u3 = random.uniform(-4, 2)\n",
    "    ur1 = random.uniform(0, 0.3) * (1 if u2 < 0 else -1)\n",
    "    ur2 = random.uniform(0, 0.3) * (1 if u1 > 0 else -1)\n",
    "    ur3 = random.uniform(-0.3, 0.3)\n",
    "    count += 1\n",
    "    csv_writer.writerow(\n",
    "        [\n",
    "            count,\n",
    "            np.around(u1, 3),\n",
    "            np.around(u2, 3),\n",
    "            np.around(u3, 3),\n",
    "            np.around(ur1, 2),\n",
    "            np.around(ur2, 2),\n",
    "            np.around(ur3, 2),\n",
    "        ]\n",
    "    )\n",
    "csv_file.close()\n",
    "print(f\"✅ Motion saved to: {motion_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of `u1`, `u2`, `u3`, `r1`, `r2`, `r3` can be adjusted according to the actual situation. The motion data will be saved as `motion.csv` in the defined path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the motion data to npy\n",
    "motion = np.loadtxt(motion_csv_path, delimiter=\",\")\n",
    "print(\"motion head:\\n\", motion[:5])\n",
    "motion_npy_path = os.path.join(data_dir, \"motion.npy\")\n",
    "np.save(motion_npy_path, motion[:, 1:])\n",
    "print(f\"✅ Motion saved to: {motion_npy_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input File Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the input files based on the motion data and the template, we need to replace lines corresponding to `BC-bound` with the following text:\n",
    "\n",
    "```text\n",
    "** Name: BC_2 Type: Displacement/Rotation\n",
    "*Boundary\n",
    "Set-RP-1, 1, 1, u1\n",
    "Set-RP-1, 2, 2, u2\n",
    "Set-RP-1, 3, 3, u3\n",
    "Set-RP-1, 4, 4, ur1\n",
    "Set-RP-1, 5, 5, ur2\n",
    "Set-RP-1, 6, 6, ur3\n",
    "```\n",
    "\n",
    "To faster the simulation, we separate the simulation into multiple parts and run them in parallel. Here is an example for generating the input files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parallel number\n",
    "parallel_number = 10\n",
    "\n",
    "# Read metaball.inp\n",
    "with open(inp_path, \"r\") as f:\n",
    "    template = f.read()\n",
    "\n",
    "# Read motion.csv\n",
    "motion_list = pd.read_csv(motion_csv_path, header=None)\n",
    "\n",
    "# Separate the motion data\n",
    "for i in range(parallel_number):\n",
    "    motion_list.iloc[\n",
    "        int(len(motion_list) / parallel_number * i) : int(\n",
    "            len(motion_list) / parallel_number * (i + 1)\n",
    "        )\n",
    "    ].to_csv(os.path.join(data_dir, f\"motion_{i + 1}.csv\"), header=None, index=None)\n",
    "\n",
    "# Generate input files\n",
    "for i in range(1, parallel_number + 1):\n",
    "    motion_i_csv_path = os.path.join(data_dir, f\"motion_{i}.csv\")\n",
    "    motion_i_list = pd.read_csv(motion_i_csv_path, header=None)\n",
    "    inp_i_dir = os.path.join(data_dir, f\"inp_{i}\")\n",
    "    os.makedirs(inp_i_dir, exist_ok=True)\n",
    "    for j in tqdm(range(len(motion_i_list)), desc=f\"Generating inputs for part {i}\", ncols=100):\n",
    "        # Read the motion data\n",
    "        count, u1, u2, u3, ur1, ur2, ur3 = motion_i_list.iloc[j, :]\n",
    "        # Replace the template with the motion data\n",
    "        inp = (\n",
    "            template.replace(\"u1\", str(u1))\n",
    "            .replace(\"u2\", str(u2))\n",
    "            .replace(\"u3\", str(u3))\n",
    "            .replace(\"ur1\", str(ur1))\n",
    "            .replace(\"ur2\", str(ur2))\n",
    "            .replace(\"ur3\", str(ur3))\n",
    "        )\n",
    "        # Write the input file\n",
    "        with open(os.path.join(inp_i_dir, f\"{int(count)}.inp\"), \"w\") as f:\n",
    "            f.write(inp)\n",
    "    # Remove the motion_i.csv file\n",
    "    os.remove(motion_i_csv_path)\n",
    "    print(f\"✅ Input files for part {i} generated in: {inp_i_dir}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following command to submit a single job:\n",
    "\n",
    "`Terminal` (Linux):\n",
    "\n",
    "```bash [Linux]\n",
    "abq job={JOB_NAME} cpus=4 int\n",
    "```\n",
    "\n",
    "`PowerShell` (Windows):\n",
    "\n",
    "```bash [Windows]\n",
    "abaqus job={JOB_NAME} cpus=4 int\n",
    "```\n",
    "\n",
    "For Abaqus 2022, the CLI is `abq` in Linux and `abaqus` in Windows, which may be different in other versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit multiple jobs in batch, we first need a `.sh` (Linux) or `.bat` (Windows) file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the `.sh` or `.bat` file to each directory containing the `.inp` files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_windows = platform.system() == \"Windows\"\n",
    "src_ext = \"bat\" if is_windows else \"sh\"\n",
    "src_file_path = os.path.join(\"..\", \"scripts\", f\"run.{src_ext}\")\n",
    "\n",
    "for i in range(1, parallel_number + 1):\n",
    "    dst_file_path = os.path.join(data_dir, f\"inp_{i}\", f\"run.{src_ext}\")\n",
    "    shutil.copy(src_file_path, dst_file_path)\n",
    "    print(f\"✅ Run script copied to: {dst_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run in the `Terminal` (Linux):\n",
    "\n",
    "```bash\n",
    "cd /path/to/each/inp_*\n",
    "sudo chmod +x run.sh\n",
    "bash run.sh\n",
    "```\n",
    "\n",
    "or `PowerShell` (Windows):\n",
    "\n",
    "```bash\n",
    "cd /path/to/each/inp_*\n",
    ".\\run.bat\n",
    "```\n",
    "\n",
    "to submit all jobs in batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will take some time to finish the simulation, so please be patient."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the simulation, we first move all file into one directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"..\", \"data\", omnineck_type, \"sim\")\n",
    "if not os.path.exists(data_dir):\n",
    "    raise FileNotFoundError(\n",
    "        f\"⚠️ Data directory '{data_dir}' does not exist. Please ensure the data is available.\"\n",
    "    )\n",
    "\n",
    "parallel_number = len([d for d in os.listdir(data_dir) if d.startswith(\"inp_\")])\n",
    "\n",
    "abq_dir = os.path.join(data_dir, \"abq_files\")\n",
    "os.makedirs(abq_dir, exist_ok=True)\n",
    "\n",
    "for i in range(1, parallel_number + 1):\n",
    "    inp_i_dir = os.path.join(data_dir, f\"inp_{i}\")\n",
    "    \n",
    "    if not os.path.exists(inp_i_dir):\n",
    "        print(f\"⚠️ Warning: source path {inp_i_dir} does not exist, skipping.\")\n",
    "        continue\n",
    "\n",
    "    for file in tqdm(os.listdir(inp_i_dir), desc=f\"Moving files for part {i}\", ncols=100):\n",
    "        src_file = os.path.join(inp_i_dir, file)\n",
    "        dst_file = os.path.join(abq_dir, file)\n",
    "\n",
    "        if os.path.exists(dst_file):\n",
    "            os.remove(dst_file)\n",
    "        shutil.move(src_file, dst_file)\n",
    "\n",
    "    try:\n",
    "        os.rmdir(inp_i_dir)\n",
    "    except OSError:\n",
    "        print(f\"⚠️ Warning: failed to remove {inp_i_dir}, not empty or in use.\")\n",
    "        \n",
    "    print(f\"✅ Input files for part {i} moved to: {abq_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we need to find out whether all simulations are completed successfully according to `.sta` files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of odb files completed successfully\n",
    "odbs = []\n",
    "\n",
    "# Check for .sta files in the abq_dir\n",
    "for file in tqdm(os.listdir(abq_dir), desc=\"Checking .sta files\", ncols=100):\n",
    "    # Only process .sta files\n",
    "    if file.endswith(\".sta\"):\n",
    "        sta_path = os.path.join(abq_dir, file)\n",
    "        try:\n",
    "            # Try reading the file with gbk encoding first\n",
    "            with open(sta_path, \"r\", encoding=\"gbk\") as f:\n",
    "                lines = f.readlines()\n",
    "        except UnicodeDecodeError:\n",
    "            # If gbk fails, try utf-8 encoding\n",
    "            with open(sta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.readlines()\n",
    "        \n",
    "        # Filter out empty lines and check for completion message\n",
    "        lines = [line.strip() for line in lines if line.strip()]\n",
    "        if lines and \"THE ANALYSIS HAS COMPLETED SUCCESSFULLY\" in lines[-1]:\n",
    "            odbs.append(file.replace(\".sta\", \".odb\"))\n",
    "\n",
    "# Save odbs to a CSV file\n",
    "odb_csv_path = os.path.join(data_dir, \"odbs.csv\")\n",
    "with open(odb_csv_path, \"w\") as f:\n",
    "    for odb in odbs:\n",
    "        f.write(odb + \"\\n\")\n",
    "print(f\"✅ List of completed odb files saved to: {odb_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we extract the data from the `.odb` files:\n",
    "\n",
    "```bash\n",
    "cd omnineck\n",
    "abaqus cae script=scripts/read_odb.py\n",
    "```\n",
    "\n",
    "The data will be saved in a new `./data/csv_files/` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that nodes without displacement need to be removed from data since they will cause errors in the training process. We load  the set of nodes witout displacement `Set-zero` in `./template/zero_nodes.txt` which is copyed from the `./template/omnineck.inp` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge all data into one `.npy` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load odbs from the CSV file\n",
    "data_dir = os.path.join(\"..\", \"data\", omnineck_type, \"sim\")\n",
    "odbs_csv_path = os.path.join(data_dir, \"odbs.csv\")\n",
    "odbs = pd.read_csv(odbs_csv_path, header=None).values.reshape(-1)\n",
    "\n",
    "# Load txt from the template\n",
    "deform_node_txt_path = os.path.join(template_dir, \"deform_node.txt\")\n",
    "surface_triangle_txt_path = os.path.join(template_dir, \"surface_triangle.txt\")\n",
    "surface_coordinate_txt_path = os.path.join(template_dir, \"surface_coordinate.txt\")\n",
    "if not os.path.exists(deform_node_txt_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"⚠️ Deform node file '{deform_node_txt_path}' does not exist. Please ensure the template is available.\"\n",
    "    )\n",
    "elif not os.path.exists(surface_triangle_txt_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"⚠️ Surface triangle file '{surface_triangle_txt_path}' does not exist. Please ensure the template is available.\"\n",
    "    )\n",
    "elif not os.path.exists(surface_coordinate_txt_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"⚠️ Surface coordinate file '{surface_coordinate_txt_path}' does not exist. Please ensure the template is available.\"\n",
    "    )\n",
    "\n",
    "deform_node = np.loadtxt(deform_node_txt_path).astype(int)\n",
    "surface_triangle = np.loadtxt(surface_triangle_txt_path, dtype=int, delimiter=\",\")\n",
    "surface_coordinate = np.loadtxt(surface_coordinate_txt_path, delimiter=\",\")\n",
    "\n",
    "csv_dir = os.path.join(data_dir, \"csv_files\")\n",
    "if not os.path.exists(csv_dir):\n",
    "    raise FileNotFoundError(\n",
    "        f\"⚠️ CSV directory '{csv_dir}' does not exist. Please ensure the CSV files are available.\"\n",
    "    )\n",
    "\n",
    "# Initialize lists to store data\n",
    "motion_list = []\n",
    "force_list = []\n",
    "surface_node_list = []\n",
    "bound_node_list = []\n",
    "\n",
    "for odb in tqdm(odbs, desc=\"Processing ODBs\", ncols=100):\n",
    "    csv_path = os.path.join(csv_dir, odb.replace(\".odb\", \".csv\"))\n",
    "    with open(csv_path, \"r\") as f:\n",
    "        lines = [line.strip().split(\",\") for line in f if line.strip()]\n",
    "\n",
    "    section_indices = {}\n",
    "    for i, row in enumerate(lines):\n",
    "        label = row[0].strip().lower()\n",
    "        if label.startswith(\"u1\"):\n",
    "            section_indices[\"motion\"] = i\n",
    "        elif label.startswith(\"sof1\"):\n",
    "            section_indices[\"force\"] = i\n",
    "        elif label.startswith(\"surface_node_label\"):\n",
    "            section_indices[\"surface_node\"] = i\n",
    "        elif label.startswith(\"bound_node_label\"):\n",
    "            section_indices[\"bound_node\"] = i\n",
    "\n",
    "    motion = np.array([float(x) for x in lines[section_indices[\"motion\"] + 1]])\n",
    "    motion_list.append(motion)\n",
    "\n",
    "    force = np.array([float(x) for x in lines[section_indices[\"force\"] + 1]])\n",
    "    force_list.append(force)\n",
    "\n",
    "    surface_start = section_indices[\"surface_node\"] + 1\n",
    "    surface_end = section_indices[\"bound_node\"]\n",
    "    surface_block = lines[surface_start:surface_end]\n",
    "\n",
    "    try:\n",
    "        filtered = [surface_block[i - 1] for i in deform_node]\n",
    "        surface_data = np.array([[float(x) for x in row] for row in filtered])\n",
    "    except IndexError:\n",
    "        raise IndexError(\"Indices in deform_node.txt are out of bounds for the surface node data.\")\n",
    "\n",
    "    surface_node_list.append(surface_data)\n",
    "\n",
    "    bound_start = section_indices[\"bound_node\"] + 1\n",
    "    bound_block = np.array([[float(x) for x in row] for row in lines[bound_start:]])\n",
    "    bound_node_list.append(bound_block)\n",
    "\n",
    "# Convert lists to arrays\n",
    "motion = np.asarray(motion_list, dtype=np.float32)          # (N, 6)\n",
    "force = np.asarray(force_list, dtype=np.float32)            # (N, 6)\n",
    "surface_node = np.asarray(surface_node_list, dtype=np.float32)   # (N, Nd, 3)\n",
    "bound_node = np.asarray(bound_node_list, dtype=np.float32)       # (N, Nb, 3)\n",
    "\n",
    "N = len(motion)\n",
    "indices = np.random.permutation(N)\n",
    "split = int(N * 0.8)\n",
    "train_idx, test_idx = indices[:split], indices[split:]\n",
    "\n",
    "h5_path = os.path.join(data_dir, \"data.h5\")\n",
    "with h5py.File(h5_path, \"w\") as f:\n",
    "    for name, idx in [(\"train\", train_idx), (\"test\", test_idx)]:\n",
    "        grp = f.create_group(name)\n",
    "\n",
    "        grp.create_dataset(\n",
    "            \"pose\", data=motion[idx],\n",
    "            chunks=(128, 6), compression=\"gzip\"\n",
    "        )\n",
    "        grp.create_dataset(\n",
    "            \"force\", data=force[idx],\n",
    "            chunks=(128, 6), compression=\"gzip\"\n",
    "        )\n",
    "        grp.create_dataset(\n",
    "            \"surface_node\", data=surface_node[idx],\n",
    "            chunks=(128, surface_node.shape[1], 3),\n",
    "            compression=\"gzip\", shuffle=True\n",
    "        )\n",
    "        grp.create_dataset(\n",
    "            \"bound_node\", data=bound_node[idx],\n",
    "            chunks=(128, bound_node.shape[1], 3),\n",
    "            compression=\"gzip\", shuffle=True\n",
    "        )\n",
    "\n",
    "    meta = f.create_group(\"meta\")\n",
    "    meta.create_dataset(\"deform_node_ids\", data=deform_node)\n",
    "    meta.create_dataset(\"surface_triangle\", data=surface_triangle)\n",
    "    meta.create_dataset(\"surface_coordinate\", data=surface_coordinate)\n",
    "    meta.attrs[\"description\"] = \"OmniNeck Abaqus simulation dataset\"\n",
    "print(f\"✅ Data saved to: {h5_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the motion, force, and surface_node data\n",
    "with h5py.File(os.path.join(data_dir, \"data.h5\"), \"r\") as data:\n",
    "    pose = data[\"train/pose\"][:]\n",
    "    force = data[\"train/force\"][:]\n",
    "    surface_node = data[\"train/surface_node\"][:]\n",
    "    bound_node = data[\"train/bound_node\"][:]\n",
    "print(\"pose shape:\", pose.shape)\n",
    "print(\"force shape:\", force.shape)\n",
    "print(\"surface_node shape:\", surface_node.shape)\n",
    "print(\"bound_node shape:\", bound_node.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have successfully prepared the dataset for training and testing the NeckNet model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model, run the following command:\n",
    "\n",
    "```bash\n",
    "python scripts/train.py [options]\n",
    "```\n",
    "\n",
    "There are several configurable options for training:\n",
    "\n",
    "| Options                | Description                                      | Type  | Default             |\n",
    "| ---------------------- | ------------------------------------------------ | ----- | ------------------- |\n",
    "| --batch-size           | Batch size for training.                         | int   | 128                 |\n",
    "| --lr                   | Learning rate for the optimizer.                 | float | 1e-5                |\n",
    "| --max-epochs           | Maximum number of training epochs.               | int   | 2000                |\n",
    "| --save-dir             | Directory to save training logs and checkpoints. | str   | lightning_logs      |\n",
    "| --data.dataset-path    | Path to the dataset directory.                   | str   | ./data/omnineck/sim |\n",
    "| --data.num-workers     | Number of workers for data loading.              | int   | 4                   |\n",
    "| --data.pin-memory      | Whether to pin memory during data loading.       | bool  | False               |\n",
    "| --data.train-val-split | Train-validation split ratios.                   | tuple | 0.875 0.125         |\n",
    "| --model.name           | Model name                                       | str   | NeckNet             |\n",
    "| --model.x-dim          | Input dimension                                  | tuple | 6                   |\n",
    "| --model.y-dim          | Output dimension                                 | tuple | 6 2862              |\n",
    "| --model.h1-dim         | Hidden layer 1 dimension                         | tuple | 128 1024            |\n",
    "| --model.h2-dim         | Hidden layer 2 dimension                         | tuple | 128 1024            |\n",
    "\n",
    "\n",
    "The logs will be saved in `./lightning_logs/` folder, and named by the timestamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the model compatible with other platforms, we can export the trained model to ONNX format. The following command will export the model to `model.onnx` file:\n",
    "\n",
    "```bash\n",
    "python scripts/export_onnx.py --ckpt_dir <ckpt_dir>\n",
    "```\n",
    "\n",
    "The `ckpt_dir` is the directory containing the checkpoint file of the trained model, which is usually in `./lightning_logs/` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we provide a simple example to test the model. We first load the model by defining the `onnx_path`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the NeckNet model\n",
    "onnx_path = \"../models/NeckNet.onnx\"\n",
    "model = NeckNetRuntime(onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we calculate the R2 score and RMSE of each modality and visualize the results with linear plots, taking the motion modality as the input and the force and shape modalities as the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "with h5py.File(os.path.join(data_dir, \"data.h5\"), \"r\") as data:\n",
    "    motion_gt = data[\"test/pose\"][:]\n",
    "    force_gt = data[\"test/force\"][:]\n",
    "    node_gt = data[\"test/surface_node\"][:, :, 1:]\n",
    "    sample_num = motion_gt.shape[0]\n",
    "print(\"motion_gt shape:\", motion_gt.shape)\n",
    "print(\"force_gt shape:\", force_gt.shape)\n",
    "print(\"node_gt shape:\", node_gt.shape)\n",
    "\n",
    "# Estimate the force and node using the model\n",
    "force_est = []\n",
    "node_est = []\n",
    "print(\"Estimating force and node using the model...\")\n",
    "for i in tqdm(range(motion_gt.shape[0]), desc=\"Estimating\", ncols=100):\n",
    "    motion = motion_gt[i]\n",
    "    force, node = model.infer(motion)\n",
    "    force_est.append(force)\n",
    "    node_est.append(node)\n",
    "force_est = np.array(force_est).reshape(-1, 6)\n",
    "node_est = np.array(node_est).reshape(sample_num, -1, 3)\n",
    "print(\"force_est shape:\", force_est.shape)\n",
    "print(\"node_est shape:\", node_est.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the r2 score function\n",
    "def r2_score(y_true: np.ndarray, y_est: np.ndarray) -> np.ndarray:\n",
    "    if y_true.ndim == 1:\n",
    "        return 1 - np.sum((y_true - y_est) ** 2) / np.sum(\n",
    "            (y_true - np.mean(y_true)) ** 2\n",
    "        )\n",
    "    else:\n",
    "        r2_list = []\n",
    "        for i in range(y_true.shape[1]):\n",
    "            r2 = 1 - np.sum((y_true[:, i] - y_est[:, i]) ** 2) / np.sum(\n",
    "                (y_true[:, i] - np.mean(y_true[:, i])) ** 2\n",
    "            )\n",
    "            r2_list.append(r2)\n",
    "        return np.array(r2_list)\n",
    "\n",
    "# Define the root mean square error (rmse) function\n",
    "def rmse(y_true: np.ndarray, y_est: np.ndarray) -> np.ndarray:\n",
    "    if y_true.ndim == 1:\n",
    "        return np.sqrt(np.mean((y_true - y_est) ** 2))\n",
    "    else:\n",
    "        rmse_list = []\n",
    "        for i in range(y_true.shape[1]):\n",
    "            rmse = np.sqrt(np.mean((y_true[:, i] - y_est[:, i]) ** 2))\n",
    "            rmse_list.append(rmse)\n",
    "        return np.array(rmse_list)\n",
    "\n",
    "# Calculate r2 score\n",
    "r2_force = r2_score(force_gt, force_est)\n",
    "r2_node = r2_score(node_gt.reshape(-1, 3), node_est.reshape(-1, 3))\n",
    "print(\"R2 score of force:\", r2_force)\n",
    "print(\"R2 score of node:\", r2_node)\n",
    "\n",
    "# Calculate rmse\n",
    "rmse_force = rmse(force_gt, force_est)\n",
    "rmse_node = rmse(node_gt.reshape(-1, 3), node_est.reshape(-1, 3))\n",
    "print(\"RMSE of force:\", rmse_force)\n",
    "print(\"RMSE of node:\", rmse_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear plot\n",
    "def gt_est_plot(gt, pred, min, max, name, color):\n",
    "    subplot_num = len(gt[0])\n",
    "    fig = plt.figure(figsize=(3 * subplot_num, 3), dpi=200)\n",
    "    for i in range(subplot_num):\n",
    "        plt.subplot(1, subplot_num, i + 1)\n",
    "        plt.plot(gt[:, i], pred[:, i], \".\", color=color, markersize=\"1\")\n",
    "        plt.plot([min[i], max[i]], [min[i], max[i]], \"k--\", linewidth=0.8, alpha=0.5)\n",
    "        plt.xlabel(\"Ground truth\")\n",
    "        plt.ylabel(\"Prediction\")\n",
    "        plt.xlim([min[i], max[i]])\n",
    "        plt.ylim([min[i], max[i]])\n",
    "        plt.xticks(np.arange(min[i], max[i] + 0.1, (-min[i] + max[i]) / 4))\n",
    "        plt.yticks(np.arange(min[i], max[i] + 0.1, (-min[i] + max[i]) / 4))\n",
    "        plt.grid()\n",
    "        plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n",
    "    fig.suptitle(name)\n",
    "    fig.tight_layout(pad=0.4, w_pad=0, h_pad=10)\n",
    "\n",
    "\n",
    "rand_num = 5000\n",
    "rand_idx_force = np.random.choice(len(force_gt), rand_num, replace=False)\n",
    "rand_idx_node = np.random.choice(len(node_gt), rand_num, replace=False)\n",
    "gt_est_plot(\n",
    "    force_gt[rand_idx_force, :3],\n",
    "    force_est[rand_idx_force, :3],\n",
    "    [-100, -100, -200],\n",
    "    [100, 100, 200],\n",
    "    \"Force\",\n",
    "    \"#38bdf6\",\n",
    ")\n",
    "gt_est_plot(\n",
    "    force_gt[rand_idx_force, 3:],\n",
    "    force_est[rand_idx_force, 3:],\n",
    "    [-4000, -4000, -2000],\n",
    "    [4000, 4000, 2000],\n",
    "    \"Torque\",\n",
    "    \"#38bdf6\",\n",
    ")\n",
    "gt_est_plot(\n",
    "    node_gt[rand_idx_node, :, :].reshape(-1, 3),\n",
    "    node_est[rand_idx_node, :, :].reshape(-1, 3),\n",
    "    [-30, -30, -25],\n",
    "    [30, 30, 25],\n",
    "    \"Shape\",\n",
    "    \"#fc3839\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also manually input the motion data and visualize the mesh in 3D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set motion and predict force and shape\n",
    "# motion = np.array([6, -6, -1, 0.2, 0.1, 0])\n",
    "motion = np.array([0, 0, 0, 0, 0, 0])\n",
    "force_infer, node_infer = model.infer(motion)\n",
    "force_infer = force_infer.reshape(-1)\n",
    "node_infer = node_infer.reshape(-1, 3)\n",
    "\n",
    "# Print force\n",
    "print(\"Inferred force:\", force_infer)\n",
    "\n",
    "# Define paths for the generated files\n",
    "surface_triangle_txt_path = os.path.join(template_dir, \"surface_triangle.txt\")\n",
    "surface_coordinate_txt_path = os.path.join(template_dir, \"surface_coordinate.txt\")\n",
    "deform_node_txt_path = os.path.join(template_dir, \"deform_node.txt\")\n",
    "\n",
    "# Load mesh and display the result\n",
    "node_ori = np.loadtxt(surface_coordinate_txt_path, delimiter=\",\")\n",
    "mesh_triangle = np.loadtxt(surface_triangle_txt_path, dtype=int, delimiter=\",\") - 1\n",
    "deform_node = np.loadtxt(deform_node_txt_path, dtype=int)\n",
    "node_curr = node_ori.copy()\n",
    "node_curr[deform_node - 1] += node_infer.reshape(-1, 3)\n",
    "mesh3d = go.Mesh3d(\n",
    "    x=node_curr[:, 0],\n",
    "    y=node_curr[:, 1],\n",
    "    z=node_curr[:, 2],\n",
    "    i=mesh_triangle[:, 0],\n",
    "    j=mesh_triangle[:, 1],\n",
    "    k=mesh_triangle[:, 2],\n",
    "    intensity=np.linalg.norm(node_curr - node_ori, axis=1),\n",
    "    colorscale=\"Viridis\",\n",
    "    autocolorscale=False,\n",
    "    colorbar=dict(\n",
    "        title=dict(\n",
    "            text=\"Displacement (mm)\",\n",
    "            side=\"right\",\n",
    "            font=dict(size=16, family=\"Arial\"),\n",
    "        ),\n",
    "        tickvals=[0, 4, 8, 12],\n",
    "        tickfont=dict(size=14, family=\"Arial\"),\n",
    "        len=0.5,\n",
    "    ),\n",
    "    cmin=0,\n",
    "    cmax=12,\n",
    "    showscale=True,\n",
    "    flatshading=True,\n",
    ")\n",
    "fig = go.Figure(data=[mesh3d])\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        camera=dict(\n",
    "            eye=dict(x=1, y=1, z=1),\n",
    "            projection=dict(type=\"orthographic\"),\n",
    "        ),\n",
    "        xaxis=dict(nticks=4, range=[-50, 50]),\n",
    "        yaxis=dict(nticks=4, range=[-50, 50]),\n",
    "        zaxis=dict(nticks=4, range=[0, 100]),\n",
    "        aspectmode=\"manual\",\n",
    "        aspectratio=go.layout.scene.Aspectratio(x=1, y=1, z=1),\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=0, pad=0),\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnineck",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
